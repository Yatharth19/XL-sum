{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XL-Sum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSrovwagRaOkvdbcY5IvQp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yatharth19/XL-sum/blob/main/XL_Sum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Bu8zNsd08vb",
        "outputId": "b48e4e84-cb9c-42cb-f6ff-a736a40d8cf4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece  #Sentencepiece is a tokenizer and we need it because our pre-trained model was tokenized using sentencepiece."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db1ACudi1y_q",
        "outputId": "8dbb32a8-9af0-48a5-e209-31e2400cee88"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING DEPENDENCIES"
      ],
      "metadata": {
        "id": "tpM3_49oWD2u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZpAjpnNu0yy6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#importing regular expression library (regex) to deal with the text\n",
        "import re \n",
        "\n",
        "#importing AutoTokenizer which will be used to download the tokenizer associated to the model we pick\n",
        "#It takes the text data and converts it into tokenized data\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#Importing HuggingFace model used\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINING MODEL NAME, TEXT, PREPROCESSING DEFINITIONS"
      ],
      "metadata": {
        "id": "wrApLfr7WKQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))  #Replaces redundant spaces with single and newline characters with space\n",
        "\n",
        "#This variable contains the text whose summary is desired\n",
        "article_text = \"\"\"To break the grip of corruption and black money, we have decided that the five hundred rupee and thousand rupee currency notes presently in use will no longer be legal tender from midnight tonight, that is 8th November 2016. This means that these notes will not be acceptable for transactions from midnight onwards. The five hundred and thousand rupee notes hoarded by anti-national and anti-social elements will become just worthless pieces of paper. The rights and the interests of honest, hard-working people will be fully protected. Let me assure you that notes of one hundred, fifty, twenty, ten, five, two and one rupee and all coins will remain legal tender and will not be affected.\"\"\"\n",
        "\n",
        "#The model name given to the pre trained model in the transformers model existing in pytorch\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n"
      ],
      "metadata": {
        "id": "xMMGg0JW00mD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING MODEL AND TOKENIZER (SENTENCEPIECE)"
      ],
      "metadata": {
        "id": "N3bPojEdWRWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Importing the model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "#The tokenization process for this specific model(csebuetnlp/mT5_multilingual_XLSum) will get downloaded\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#This tokenizer is responsible for converting the textual data in numerical values. \n"
      ],
      "metadata": {
        "id": "VI8osGYJUnnP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "READING THE INPUT"
      ],
      "metadata": {
        "id": "TSYQCJ9lWXQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_ids = tokenizer(                  # Tokenizing a text is splitting it into words or subwords, which then are converted to ids through a look-up table\n",
        "    [WHITESPACE_HANDLER(article_text)], # The input fed to the model after handling white spaces from the text\n",
        "    return_tensors=\"pt\",                # It will return tensors instead of list of python integers\n",
        "    padding=\"max_length\",               # To make all the sentences read of the maximum length\n",
        "    truncation=True,                    # In case of any space, it will get removed\n",
        "    max_length=512                      # The maximum length for a particular sentence will be 512 \n",
        ")[\"input_ids\"]\n"
      ],
      "metadata": {
        "id": "VW-phaNHUsjc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRODUCING THE OUTPUT TOKENS"
      ],
      "metadata": {
        "id": "ooaTXOKIWaA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,                # The data for which output has to be generated\n",
        "    max_length=84,                      # The maximum allowed length that can be printed in the summary\n",
        "    no_repeat_ngram_size=2,             # It makes sure that no word sequences of 2 words appears twice by manually setting its probability to zero\n",
        "    num_beams=4                         # It always gives that output where the probability of individual words is maximum\n",
        ")[0]\n"
      ],
      "metadata": {
        "id": "AKdwtuftUuaN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRINTING THE OUTPUT AS TEXT"
      ],
      "metadata": {
        "id": "nXm2lTlnWh5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summary = tokenizer.decode(             # Command to decode the output produced\n",
        "    output_ids,                         # The output ids produced\n",
        "    skip_special_tokens=True,           # It decides whether or not to remove special tokens in the decoding.\n",
        "    clean_up_tokenization_spaces=False  # Whether or not to clean up the tokenization spaces.\n",
        ")\n",
        "\n",
        "print(summary)                          # The produced summary of the long text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myHoKupFUvw6",
        "outputId": "2e90e93b-a441-41ba-b2ef-ca8cd01e978a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "India's currency notes have been banned from legal tender from 8 November.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "87O9HjdnU270"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}