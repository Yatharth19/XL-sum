# XL-sum

Automatic text summarization is a fundamental problem in natural language processing. Given an input text (typically a long document or article), the goal is to generate a smaller, concise piece of text that conveys the key information of the input text.

This Github repository implements the code as per in the research paper (https://aclanthology.org/2021.findings-acl.413.pdf).

# Workflow

# Im Here

# Im here Again !!

1. Firstly, all the dependencies are installed. (I am using transformers for this NLP task).
2. The complete text to be summarized and the pre-trained model name is defined next.
3. The model and tokenizer used while training the model are downloaded and used within the input text.
4. Output tokens are produced as per various optimal parameters.
5. The produced tokens are then decoded into text to produce a short summary of the whole text.

# TRANSFORMERS

A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP) and computer vision (CV).
It is a part of the HuggingFaceðŸ¤— library
